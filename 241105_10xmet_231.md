Use 10x cellranger for initial fastq generation

```bash
#set up environment variables 
export SCRATCH="/volumes/USR2/Ryan/work"
export projDir="/volumes/USR2/Ryan/projects/10x_MET"
export srcDir="${projDir}/src"
export refDir="/volumes/USR2/Ryan/ref/refdata-cellranger-arc-GRCh38-2020-A-2.0.0"
export outDir="${projDir}/241105_10xmet_231"
export DNArunDir="/volumes/seq/flowcells/MDA/nextseq2000/2024/241103_RM_10xmet_atrandi_met/241103_VH01788_59_AAG2N3JM5"
export outname="10xmet_231"
export task_cpus=250
export index="/volumes/USR2/Ryan/projects/metact/ref/bsbolt/genome_index.pkl"

mkdir -p $outDir
cd $outDir

echo """Lane,Sample,Index
*,${outname}_vera_noconv,SI-NA-G1
*,${outname}_vera_conv,SI-NA-G3
*,${outname}_dmet_noconv,SI-NA-G2
*,${outname}_dmet_conv,SI-NA-G4""" > sample_met.csv
export sample_met="${outDir}/sample_met.csv"

mkdir -p ${SCRATCH}/10xmet_work

~/tools/cellranger-atac-2.1.0/cellranger-atac mkfastq \
--run=${DNArunDir} \
--csv=${sample_met} \
--jobmode="local" \
--localcores=${task_cpus} \
--delete-undetermined \
--output-dir="${outDir}" \
--id=${outname}_met



```
Set up fastq files with indexes for alignment
singularity shell ~/singularity/amethyst.sif

```python
import gzip
from Bio import SeqIO
import sys, os, re, time
from Bio.SeqIO.QualityIO import FastqGeneralIterator
from Bio.Seq import Seq
import pandas as pd
from multiprocessing.pool import ThreadPool
from scipy.spatial import distance
import argparse
import subprocess
import glob

os.chdir("/volumes/USR2/Ryan/projects/10x_MET/241105_10xmet_231/AAG2N3JM5/10xmet_231_vera_conv")

parser = argparse.ArgumentParser()
parser.add_argument('--read_chunks',default="tmp_read")
parser.add_argument('--index_chunks',default="tmp_idx")
parser.add_argument('--barcode_whitelist',default="/volumes/USR2/Ryan/tools/cellranger-atac-2.1.0/lib/python/atac/barcodes/737K-cratac-v1.txt.gz")
parser.add_argument('--idx_count',default="idx_count.txt")
parser.add_argument('--num_threads',default = 50)
args = parser.parse_args()

barc=pd.read_csv(args.barcode_whitelist,compression='gzip',header=None)[0].tolist()
barc=[str(Seq(i).reverse_complement()) for i in barc]
start_time = time.time()
index_count=pd.read_table(args.idx_count,names=['count','cellid'],sep="\t")
index_count_filt= index_count[index_count['count'] > 1000] 
#check barcode against white list to filter (removes a lot of the multi Gs)
cellid_whitelist=[i for i in index_count_filt['cellid'].tolist() if i in barc]

r1=sorted(glob.glob(args.read_chunks+"/*_R1_*fastq.gz"))
r2=sorted(glob.glob(args.read_chunks+"/*_R3_*fastq.gz"))
idx3=sorted(glob.glob(args.index_chunks+"/*_I1_*fastq.gz"))
idx4=sorted(glob.glob(args.index_chunks+"/*_R2_*fastq.gz"))

def filter_chunk(chunk):
    # Process each chunk here
    i=0
    r1,r2,idx3,idx4=chunk
    j=r1[len(r1)-12:-9]
    fq1_outname=r1[:-9]+".barc.fastq.gz"
    fq2_outname=r2[:-9]+".barc.fastq.gz"
    with gzip.open(fq1_outname, "wb") as outfile_fq1, \
    gzip.open(fq2_outname, "wb") as outfile_fq2:
        for ((title1, seq1, qual1),\
        (title2, seq2, qual2),\
        (title3, seq3, qual3),\
        (title4, seq4, qual4)) in \
        zip(FastqGeneralIterator(gzip.open(r1,"rt")),\
            FastqGeneralIterator(gzip.open(r2,"rt")),\
            FastqGeneralIterator(gzip.open(idx3,"rt")),\
            FastqGeneralIterator(gzip.open(idx4,"rt"))):
            sample_idx=seq3[:10].strip()
            gem_idx=seq4[:16].strip()
            if gem_idx in cellid_whitelist:
                i+=1
                cell_idx=sample_idx+"_"+gem_idx
                outfile_fq1.write("@%s:%s\n%s\n+\n%s\n" % (cell_idx.strip(), str(j)+"_"+str(i), seq1.strip(), qual1.strip()))
                outfile_fq2.write("@%s:%s\n%s\n+\n%s\n" % (cell_idx.strip(), str(j)+"_"+str(i), seq2.strip(), qual2.strip()))
            else:
                hamming_distance=[round(distance.hamming(list(gem_idx), list(x)) * len(gem_idx)) for x in cellid_whitelist]
                if hamming_distance[hamming_distance.index(min(hamming_distance))]<2:
                    gem_idx=cellid_whitelist[hamming_distance.index(min(hamming_distance))] #output corrected gem_idx
                    i+=1
                    cell_idx=sample_idx+"_"+gem_idx
                    outfile_fq1.write("@%s:%s\n%s\n+\n%s\n" % (cell_idx.strip(), str(j)+"_"+str(i), seq1.strip(), qual1.strip()))
                    outfile_fq2.write("@%s:%s\n%s\n+\n%s\n" % (cell_idx.strip(), str(j)+"_"+str(i), seq2.strip(), qual2.strip()))

if __name__ == "__main__":
    with ThreadPool(int(args.num_threads)) as pool:
        pool.map(filter_chunk,zip(r1,r2,idx3,idx4))
        pool.close()
        pool.join()

```


# Run fastq barcode generator into readnames

```bash
cd ${outDir}/AAG2N3JM5
singularity shell \
--bind /volumes/USR2/Ryan/projects/metact/ref/:/ref \
~/singularity/amethyst.sif

split_fastq(){
    sample_name=${1}
    sample_number=${2}
    multicore=${3}

    /volumes/USR2/Ryan/bin/seqkit split2 \
    -1 ${sample_name}/${sample_name}_${sample_number}_L001_R1_001.fastq.gz -2 ${sample_name}/${sample_name}_${sample_number}_L001_R3_001.fastq.gz \
    -p ${multicore} -j ${multicore} -O ${sample_name}/tmp_read -f -e .gz
    /volumes/USR2/Ryan/bin/seqkit split2 \
    -1 ${sample_name}/${sample_name}_${sample_number}_L001_I1_001.fastq.gz -2 ${sample_name}/${sample_name}_${sample_number}_L001_R2_001.fastq.gz \
    -p ${multicore} -j ${multicore} -O ${sample_name}/tmp_idx -f -e .gz

    zcat ${sample_name}/${sample_name}_${sample_number}_L001_R2_001.fastq.gz \
    | awk 'NR%4==2' \
    | sort --parallel=${multicore} \
    | uniq -c | sort -k1,1n \
    | awk 'OFS="\t" {print $1,$2}' > ${sample_name}/idx_count.txt

    python $srcDir/fastqsplitter.10xmet.py \
    --read_chunks $(pwd)/${sample_name}/tmp_read \
    --index_chunks $(pwd)/${sample_name}/tmp_idx \
    --idx_count $(pwd)/${sample_name}/idx_count.txt \
    --num_threads ${multicore}

    cat $(ls $(pwd)/${sample_name}/tmp_read/*_R1_*barc.fastq.gz) > $(pwd)/${sample_name}/${sample_name}_R1.barc.fastq.gz
    cat $(ls $(pwd)/${sample_name}/tmp_read/*_R3_*barc.fastq.gz) > $(pwd)/${sample_name}/${sample_name}_R3.barc.fastq.gz
}

export split_fastq
cd ${outDir}/AAG2N3JM5
split_fastq 10xmet_231_dmet_conv S4 100
split_fastq 10xmet_231_dmet_noconv S3 100
split_fastq 10xmet_231_vera_conv S2 100
split_fastq 10xmet_231_vera_noconv S1 100

```
# Run adapter trimming and alignment
```bash
singularity shell \
--bind ~/projects \
--bind /volumes/USR2/Ryan/projects/metact/ref/:/ref \
~/singularity/scalemethyl_v1.6.sif

cd ${outDir}/AAG2N3JM5
# run fastqc to assess conversion rate
fastqc -t 100 ./*/*fastq.gz
#from fastqc we can see that genomic dna is converted compared to nonconverted strand! (yay!) and we need to trim some adapters

#run demultiplexing
#NOTE IN PYTHON SCRIPT I ACTUALLY DIDN'T WRITE IT OUT AS GZ SO JUST CORRECTING
adapter_trim() { 
sample_name=${1}

#correct output mistake (properly gzip)
mv ${sample_name}_R1.barc.fastq.gz ${sample_name}_R1.barc.fastq
mv ${sample_name}_R3.barc.fastq.gz ${sample_name}_R3.barc.fastq
for i in *barc.fastq; do gzip $i & done && wait

/opt/conda/envs/scaleMethylTools/bin/cutadapt \
-j 10 \
-a CTGTCTCTTA -A CTGTCTCTTA \
-U 10 -u 10 \
-o ${sample_name}_R1.barc.trim.fastq.gz \
-p ${sample_name}_R3.barc.trim.fastq.gz \
${sample_name}_R1.barc.fastq.gz \
${sample_name}_R3.barc.fastq.gz
}

export -f adapter_trim

#cd $outDir/AAG2N3JM5/10xmet_231_dmet_conv ; adapter_trim 10xmet_231_dmet_conv &
#cd $outDir/AAG2N3JM5/10xmet_231_dmet_noconv  ; adapter_trim 10xmet_231_dmet_noconv
#cd $outDir/AAG2N3JM5/10xmet_231_vera_conv ; adapter_trim 10xmet_231_vera_conv
#cd $outDir/AAG2N3JM5/10xmet_231_vera_noconv ; adapter_trim 10xmet_231_vera_noconv
cd $outDir

align() {
sample_name=${1}
bsbolt Align \
-F1 $(pwd)/${sample_name}/${sample_name}_R1.barc.trim.fastq.gz \
-F2 $(pwd)/${sample_name}/${sample_name}_R3.barc.trim.fastq.gz \
-t 50 -OT 50 \
-O ${sample_name} \
-DB $index >> ${sample_name}.bsbolt.log 2>> ${sample_name}.bsbolt.log
}

export -f align
export index="/ref/bsbolt"

#align 10xmet_231_dmet_conv &
#align 10xmet_231_dmet_noconv &
#align 10xmet_231_vera_conv &
#align 10xmet_231_vera_noconv &

#Add cellID to bam. note i haven't done any hamming distance error correction to the whitelist
#filter out non read pairs, and not primary alignments
#then split files by CB values

singularity shell \
--bind /volumes/USR2/Ryan/projects/metact/ref/:/ref \
~/singularity/amethyst.sif
source ~/.bashrc
#read is mapped, read is primary alignment
add_cb_split_to_sc() {
    bam=$1
    samtools view -F 260 -h -@ 20 ${bam} \
    | awk 'OFS="\t" {if ($1~ /^@/) {print $0} else {split($1,a,":"); split(a[1],b,"_"); print $0,"CB:Z:"b[2]}}' \
    | samtools sort -@ 20 \
    | samtools view -@ 20 -bT /ref/grch38/Homo_sapiens.GRCh38.dna.primary_assembly.fa -o ${bam::-4}.CB.bam - 

    output_dir=$(echo "${bam::-4}_sc")
    mkdir -p ${output_dir} 

    samtools split -d CB \
    -u ${output_dir}'/leftover.bam' \
    -M 6000 --output-fmt BAM \
    -f ${output_dir}'/%*_%!.%.' ${bam::-4}.CB.bam
    }

export -f add_cb_split_to_sc 
add_cb_split_to_sc 10xmet_231_dmet_conv.bam
add_cb_split_to_sc 10xmet_231_vera_conv.bam & 
add_cb_split_to_sc 10xmet_231_dmet_noconv.bam & #running
add_cb_split_to_sc 10xmet_231_vera_noconv.bam &


#measure of proper conversion
singularity shell \
--bind ~/projects \
--bind /volumes/USR2/Ryan/projects/metact/ref/:/ref \
--bind /usr/local/bin/ \
~/singularity/scalemethyl_v1.6.sif
source ~/.bashrc

#set up environment variables 
export SCRATCH="/volumes/USR2/Ryan/work"
export projDir="/volumes/USR2/Ryan/projects/10x_MET"
export srcDir="${projDir}/src"
export refDir="/volumes/USR2/Ryan/ref/refdata-cellranger-arc-GRCh38-2020-A-2.0.0"
export outDir="${projDir}/241105_10xmet_231"
export DNArunDir="/volumes/seq/flowcells/MDA/nextseq2000/2024/241103_RM_10xmet_atrandi_met/241103_VH01788_59_AAG2N3JM5"
export outname="10xmet_231"
export task_cpus=50
export index="/volumes/USR2/Ryan/projects/metact/ref/bsbolt"
#export index="/ref/bsbolt"



#count of aligned reads
count_bam_reads(){
    baseout=$(basename $1)
    outname=$(echo ${baseout::-4} | awk '{split($0,a,"_");print a[5]}')
    total_read_count=$(samtools view $1 | wc -l)
    uniq_read_count=$(samtools view $1 | awk 'OFS="\t" { print $4,$5}' | sort | uniq -c | wc -l)
    echo $outname,$total_read_count,$uniq_read_count
    }

met_bam_count(){
    bam=$1
    baseout=$(basename $bam)
    outname=$(echo ${baseout::-4} | awk '{split($0,a,"_");print a[5]}')
    samtools sort -n $bam | samtools fixmate -p -m - - | samtools sort | samtools markdup -r -S  --write-index - ${bam::-4}.bbrd.bam
    bsbolt CallMethylation -CG -t 1 -I ${bam::-4}.bbrd.bam -O ${bam::-4} -DB $index -min 0 > ${bam::-4}.metreport.txt
    met_cg_cov=$(grep "Methylated CpG Cytosines:" ${bam::-4}.metreport.txt | awk '{split($0,a,": ");print a[2]}')
    total_cg_cov=$(grep "Total Observed CpG Cytosines:" ${bam::-4}.metreport.txt | awk '{split($0,a,": ");print a[2]}')
    met_ch_cov=$(grep "Methylated CH Cytosines:" ${bam::-4}.metreport.txt | awk '{split($0,a,": ");print a[2]}')
    total_ch_cov=$(grep "Total Observed CH Cytosines:" ${bam::-4}.metreport.txt | awk '{split($0,a,": ");print a[2]}')
    echo $outname,$met_cg_cov,$total_cg_cov,$met_ch_cov,$total_ch_cov
    }

export -f count_bam_reads
export -f met_bam_count


# sample_name="10xmet_231_vera_noconv"
# for i in $(find ${PWD}/${sample_name}_sc -name "*CB*[A|T|C|G].bam"); do echo $i ; done > ${sample_name}.sc_list.txt
# parallel -j 20 -a ${sample_name}.sclist.txt count_bam_reads > ${sample_name}.aligned_counts.txt
# parallel -j 20 -a ${sample_name}.sc_list.txt met_bam_count > ${sample_name}.met_count.txt 

# sample_name="10xmet_231_vera_conv"
# for i in $(find ${PWD}/${sample_name}_sc -name "*CB*[A|T|C|G].bam"); do echo $i ; done > ${sample_name}.sc_list.txt
# parallel -j 20 -a ${sample_name}.sc_list.txt count_bam_reads > ${sample_name}.aligned_counts.txt
# parallel -j 20 -a ${sample_name}.sc_list.txt met_bam_count > ${sample_name}.met_count.txt 

sample_name="10xmet_231_dmet_conv"
for i in $(find ${PWD}/${sample_name}_sc -name "*CB*[A|T|C|G].bam"); do echo $i ; done > ${sample_name}.sc_list.txt
parallel -j 20 -a ${sample_name}.sc_list.txt count_bam_reads > ${sample_name}.aligned_counts.txt
parallel -j 20 -a ${sample_name}.sc_list.txt met_bam_count > ${sample_name}.met_count.txt 

sample_name="10xmet_231_dmet_noconv"
for i in $(find ${PWD}/${sample_name}_sc -name "*CB*[A|T|C|G].bam"); do echo $i ; done > ${sample_name}.sc_list.txt
parallel -j 20 -a ${sample_name}.sc_list.txt count_bam_reads > ${sample_name}.aligned_counts.txt
parallel -j 20 -a ${sample_name}.sc_list.txt met_bam_count > ${sample_name}.met_count.txt 


